{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Quantum Autoencoders"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## What's a quantum autoencoder and why should you care?\n",
    "\n",
    "Quantum autoencoders are awesome but you may be wondering what's even a normal autoencoder and what benefits come from it. To answer this we must give a brief overview of classical autoencoders. In the deep learning field it is common to see/use huge neural networks (a neural network it's just a set of interconnected neurons) to achive a goal (classify images, generate images, create music, etc..). But what if we could achive the same goal with fewer neurons? That would be awesome. Well, we can achive that by using an autoencoder. That is to generate a smaller neural network while getting the same results approximately.  "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, a quantum autoencoder applies the same principle of a classical autoencoder but instead of applying the process to neurons we apply the process to a statevector. So informally, a quantum state autoencoder is a circuit that takes a statevector as input and it outputs a reduced version of that statevector. And to get the original statevector (approximately) from the encoded statevector we can apply the opposite of a QAE, that is, a Quantum state decoder. This is a great idea because as you might know we don't get access to a lot of resources in NISQ machines so by applying a QAE we can reduce the use of those resources. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Graphically, a quantum state autoencoder and decoder can be seen as the following:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![autoencoder](./autoencoder.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "As you can see in the image, we have a 4x4 statevector that we want to encode into a 2x2 statevector. We can do this by applying the autoencoder to our circuit. One thing worth noticing is that two qubits were set to |0> in the process. Nonetheless, there wasn't a loss of information because we encoded that information inside the two last qubits. Now, if we want to have the original 4x4 statevector we need to apply the decoder to our circuit. Notice that we have to include the qubits that were set in the |0> state."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The general process to construct a quantum autoencoder is:\n",
    "- Generate a statevector that we want to reduce.\n",
    "- Create an anzast for a set of parameters (this is represented in the image as the AUTOENCODER).\n",
    "- Create the decoder by finding the inverse of the AUTOENCODER anzast.\n",
    "- Get the cost of the AUTOENCODER by comparing the original state with the restored state from the DECODER.\n",
    "- Optimize the parameters by using a classical optimization routine (ADAM, ADA, Stochastic gradient descent, etc..)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Objectives\n",
    "Now that we saw what is and why autoencoders matter we propose in the following post how to construct one, step by step. And even further, We are going to apply it to a variational circuit that classifies images to show the capabilities of a quantum autoencoder. So the contents of this post are:\n",
    "- First, we present the statevector creation that we want to reduce.\n",
    "- Secondly, We present how to build a quantum autoencoder.\n",
    "- Then, We present the decoder circuit.\n",
    "- After that, We present a comparison between different results for the QAE using different cost functions.\n",
    "- Then, We present the quantum classifier and the results from a classification.\n",
    "- Finally, we make some conclusions and present the list of references."
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}